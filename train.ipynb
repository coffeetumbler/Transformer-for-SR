{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "#from pickletools import optimize\n",
    "sys.path.append(os.path.dirname(os.path.abspath('./train.ipynb')))\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import transformer\n",
    "from models import whole_models\n",
    "from models import submodels\n",
    "#dataloader의 정확한 위치를 어디로 하실꺼죠? 그거에 따라서 좀 조절할필요가 있을 거 같아요! \n",
    "from utils.dataloader import get_dataloader\n",
    "from utils import config\n",
    "from utils import image_processing\n",
    "\n",
    "from tqdm import tqdm\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import easydict\n",
    "\n",
    "#Basic Setting : 임의로 정했습니다.\n",
    "args = easydict.EasyDict({\n",
    "        \"gpu\" : 0,\n",
    "        \"learing_rate\": 1e-4,\n",
    "        \"batch_size\" : 16,\n",
    "        \"num_epochs\" : 100,\n",
    "        \"loss\" : 'l1',\n",
    "        \"weight_decay\" : 1e-4,\n",
    "        \"upscale\" : 2,\n",
    "        \"Optimizer\" : \"AdamW\",\n",
    "        \"CosineLRScheduler\":'''[lr_min=args.learing_rate*0.01,\n",
    "        \\t\\twarmup_lr_init=args.learing_rate*0.001,\n",
    "        \\t\\twarmup_t=max_iter//10,cycle_limit=1,\n",
    "        \\t\\tt_in_epochs=False]'''\n",
    "    })\n",
    "\n",
    "#Date\n",
    "now=time.localtime()\n",
    "ntime=\"%04d%02d%02d_%02d%02d\"%(now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min)\n",
    "\n",
    "#Save Path\n",
    "save_path ='./logs'\n",
    "save_path_upscale = save_path+'/x'+str(args.upscale)\n",
    "save_path_date = save_path_upscale+'/'+ntime\n",
    "save_path_state_dict = save_path_date+'/state_dict'\n",
    "save_path_model = save_path_date+'/model'\n",
    "save_path_output = save_path_date+'/output'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "if not os.path.exists(save_path_upscale):\n",
    "    os.mkdir(save_path_upscale)\n",
    "if not os.path.exists(save_path_date):\n",
    "    os.mkdir(save_path_date)\n",
    "else: #To prevent overwrite. Ex. A, A(0), A(1), ...\n",
    "    for i in range(500):\n",
    "        save_path_date=save_path_date+\"(\"+str(i)+\")\"\n",
    "        if not os.path.exists(save_path_date):\n",
    "            os.mkdir(save_path_date)\n",
    "            break\n",
    "        save_path_date = save_path_upscale+'/'+ntime\n",
    "if not os.path.exists(save_path_state_dict):\n",
    "    os.mkdir(save_path_state_dict)\n",
    "if not os.path.exists(save_path_model):\n",
    "    os.mkdir(save_path_model)\n",
    "if not os.path.exists(save_path_output):\n",
    "    os.mkdir(save_path_output)\n",
    "\n",
    "#hyperparameters save\n",
    "file = open(save_path_date+\"/hyperparameters.txt\", \"w\")\n",
    "for code, name in args.items():\n",
    "    file.write(f'{code} : {name}\\n')\n",
    "'''file.write(str(args))'''\n",
    "file.close()\n",
    "\n",
    "#GPU setting \n",
    "cudnn.benchmark=True #input size stable -> Good\n",
    "device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Model setting & save\n",
    "model = whole_models.SRTransformer(upscale=args.upscale)\n",
    "model.to(device)\n",
    "torch.save(model, os.path.join(save_path_model+'model.pt'))\n",
    "\n",
    "#Loss setting\n",
    "if args.loss == 'l1':\n",
    "    criterion = nn.L1Loss().to(device)\n",
    "elif args.loss == 'mse':\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "\n",
    "#optimizer : 임의로 정했습니다.\n",
    "optimizer = optim.AdamW(params = model.parameters(), lr=args.learing_rate, weight_decay = args.weight_decay)\n",
    "\n",
    "#Dataset\n",
    "dataloader_train = get_dataloader(setting =\"train\", num_workers=4)\n",
    "train_size = len(dataloader_train)\n",
    "max_iter = (train_size//args.batch_size + 1) * args.num_epochs\n",
    "\n",
    "dataloader_val= get_dataloader(setting =\"valid\", num_workers=4)\n",
    "val_size = len(dataloader_val)\n",
    "\n",
    "#scheduler : warmup + cosin lr decay : 임의로 정했습니다.\n",
    "scheduler = CosineLRScheduler(optimizer,\n",
    "                              t_initial=max_iter,\n",
    "                              lr_min=args.learing_rate*0.01,\n",
    "                              warmup_lr_init=args.learing_rate*0.001,\n",
    "                              warmup_t=max_iter//10,\n",
    "                              cycle_limit=1,\n",
    "                              t_in_epochs=False\n",
    "                            )\n",
    "\n",
    "#For validation report\n",
    "min_valid_loss = 1000\n",
    "min_valid_epoch = 0\n",
    "_iter = 0\n",
    "\n",
    "def PSNR(A, B):\n",
    "    length=len(A)\n",
    "    psnr=0\n",
    "    for i in range(length):\n",
    "        mse=nn.MSELoss().to(device)(A[i], B[i])\n",
    "        psnr+=(-10)*torch.log10(mse)\n",
    "    return psnr\n",
    "\n",
    "#Training & Validation\n",
    "for epoch in range(args.num_epochs):\n",
    "\n",
    "    #Train\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm(len(train_size)) as t:\n",
    "        t.set_description('epoch : {}/{}'.format(epoch, args.num_epochs-1))\n",
    "\n",
    "        for batch, items in enumerate(dataloader_train) :\n",
    "            n_batch = items['origin'].size()[0]\n",
    "\n",
    "            items['origin'].to(device)\n",
    "            items['degraded'].to(device)\n",
    "            items['interpolated'].to(device)\n",
    "\n",
    "            mid_result=model(items['degraded'], items['interpolated'])\n",
    "            mid_result_denorm=image_processing.denormalize(mid_result, device=device) #device cpu 해 놓은 이유!\n",
    "            loss = criterion(mid_result, items['origin'])\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm(model.parameters(), 0.1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            _iter+=1\n",
    "            scheduler.step_update(_iter)\n",
    "            t.update(n_batch)\n",
    "            \n",
    "            if batch %100 ==0:\n",
    "                loss, current=loss.item(), batch*n_batch\n",
    "                tqdm.write(f\"loss: {loss:>6f}, [{current:>5d}/{train_size:>5d}]\")\n",
    "    \n",
    "    torch.save(model.state_dict(), os.path.join(save_path_state_dict+'state_dict_epoch_{}.pt'.format(epoch)))\n",
    "    \n",
    "    #Validate\n",
    "    model.eval()\n",
    "    loss_val=0\n",
    "    with torch.no_grad():\n",
    "        for batch, items in enumerate(dataloader_val):\n",
    "            n_batch = items['origin'].size()[0]\n",
    "\n",
    "            items['origin'].to(device)\n",
    "            items['degraded'].to(device)\n",
    "            items['interpolated'].to(device)\n",
    "            \n",
    "            mid_result=model(items['degraded'], items['interpolated'])\n",
    "            mid_result_denorm=image_processing.denormalize(mid_result, device=device)\n",
    "            loss_val = PSNR(mid_result_denorm, items['origin'])\n",
    "            \n",
    "            if loss_val <= min_valid_loss:\n",
    "                min_valid_loss=loss_val\n",
    "                min_valid_epoch = epoch\n",
    "        print('validation : {}\\n'.format(loss_val))\n",
    "\n",
    "print('minimum validation {} at epoch {}'.format(min_valid_loss, min_valid_epoch))\n",
    "file_rep = open(save_path_date+\"/min_val_epoch.txt\", \"w\")\n",
    "file_rep.write(f'minimum validation {min_valid_loss} at epoch {min_valid_epoch}')\n",
    "file_rep.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easydict\n",
      "  Downloading easydict-1.9.tar.gz (6.4 kB)\n",
      "Building wheels for collected packages: easydict\n",
      "  Building wheel for easydict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6360 sha256=2f8a3c514b2511c80f578e57bf68e9a13acdbf53657a44477c72ba06f151cc20\n",
      "  Stored in directory: /home/dlxorud1231/.cache/pip/wheels/fd/d2/35/4c11d19a72280492846f4c4df975311a2bac475e8021f86c1d\n",
      "Successfully built easydict\n",
      "Installing collected packages: easydict\n",
      "Successfully installed easydict-1.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install easydict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('SAM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fc4c2ba6fd48c6b68dcc57b05a8b3d8d5d405fe6f918eea8f078ca25ff586b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
