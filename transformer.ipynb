{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a10862e-db06-4650-afba-ba3fbc622d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lahj91/anaconda3/envs/SR/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import pytorch_model_summary\n",
    "\n",
    "import utils.functions as fns\n",
    "from models.transformer import MultiHeadAttentionLayer, MultiHeadSelfAttentionLayer\n",
    "from models.transformer import get_transformer_encoder, get_transformer_decoder\n",
    "from models.whole_models import SRTransformer, TwoStageDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34724ee6-9105-495d-bb71-738a5a59d88e",
   "metadata": {},
   "source": [
    "## Window Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a14b44-2a8b-4ec1-9dfc-8747f972e557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4-class cyclic shifting\n",
    "split_heads = torch.arange(72).view(2, 6, 6, 1)\n",
    "split_heads = split_heads.expand(-1, -1, -1, 8)\n",
    "split_heads = split_heads.view(2, 6, 6, 4, 2).permute(0, 3, 1, 2, 4).contiguous()\n",
    "\n",
    "shifted_heads = fns.cyclic_shift(split_heads, 1)\n",
    "# shifted_heads = shifted_heads.permute(0, 2, 3, 1, 4).contiguous().view(2, 6, 6, -1)\n",
    "print(shifted_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168a5e8-108b-49ac-bcf7-1bbbd46c1e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# window partitioning\n",
    "# shifted_heads = shifted_heads.view(2, 6, 6, 4, 2).permute(0, 3, 1, 2, 4).contiguous()\n",
    "partitions = fns.partition_window(shifted_heads, 2)\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1817c246-9f37-482f-97cb-0fa93fba575c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# window merging\n",
    "merged = fns.merge_window(partitions, 2)\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bf687-6d1d-451d-a948-83b504302786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make masking matrix.\n",
    "mask = fns.masking_matrix(4, 6, 6, 2, 1)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47831f-2ec1-4aee-b585-3e4bb1d95a51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Masking\n",
    "attn_values = torch.matmul(partitions, partitions.transpose(-1, -2))\n",
    "attn_values.masked_fill_(mask, -1)\n",
    "print(attn_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f4248-20b9-4e4a-a9d6-2282bcde67b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make masking matrix when query != key.\n",
    "mask = fns.masking_matrix(4, 8, 8, 4, 2,\n",
    "                             4, 4, 2, 1)\n",
    "print(mask[0, 1], '\\n')\n",
    "print(mask[0, 2], '\\n')\n",
    "print(mask[0, 3], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c693c-8674-4ef3-95e7-e9b9c951fda5",
   "metadata": {},
   "source": [
    "## 2D Relative Position Bias (for Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37956e12-957c-4a7b-9661-779293301dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example window index\n",
    "window_size = 3\n",
    "coord_index = np.arange(window_size*window_size).reshape((window_size, window_size))\n",
    "print(coord_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998be6a8-e843-4687-bba5-a18f549a966f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coordinate indices along each axis\n",
    "axis_size = window_size * 2 - 1\n",
    "coord_x = np.repeat(np.arange(window_size) * axis_size, window_size)\n",
    "coord_y = np.tile(np.arange(window_size), window_size)\n",
    "print(coord_x)\n",
    "print(coord_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789fc51-0926-4c98-8b35-3837ba45ddaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Relative coordinate indices along each axis\n",
    "relative_x = coord_x[:, np.newaxis] - coord_x\n",
    "relative_y = coord_y[:, np.newaxis] - coord_y\n",
    "print(relative_x)\n",
    "print(relative_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09112f3b-e885-4408-9b46-53d1b519aa7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Relative coordinate indices in 2D window\n",
    "relative_coord = relative_x + relative_y\n",
    "relative_coord += relative_coord[-1, 0]\n",
    "print(relative_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6275c18-e44f-4c6d-a974-7b062e00b291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defined function\n",
    "print(fns.relative_position_index(2).reshape((4, 4)))  # 2x2 window\n",
    "print(fns.relative_position_index(3).reshape((9, 9)))  # 3x3 window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0bc559-299a-4fbd-9e9b-abd87ea0bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example window index when key != query\n",
    "query_window_size = 4\n",
    "key_window_size = 2\n",
    "qk_ratio = query_window_size // key_window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d34498-000a-4221-9dc7-a73cd0067b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinate indices along each axis\n",
    "axis_size = query_window_size * 2 - qk_ratio\n",
    "\n",
    "query_coord_x = np.repeat(np.arange(query_window_size) * axis_size, query_window_size)\n",
    "query_coord_y = np.tile(np.arange(query_window_size), query_window_size)\n",
    "print(query_coord_x)\n",
    "print(query_coord_y)\n",
    "\n",
    "key_coord_x = np.repeat(np.arange(key_window_size) * axis_size * qk_ratio, key_window_size)\n",
    "key_coord_y = np.tile(np.arange(key_window_size) * qk_ratio, key_window_size)\n",
    "print(key_coord_x)\n",
    "print(key_coord_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ad06d-b822-4059-8453-0f85a0a4e7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Relative coordinate indices along each axis\n",
    "relative_x = query_coord_x[:, np.newaxis] - key_coord_x\n",
    "relative_y = query_coord_y[:, np.newaxis] - key_coord_y\n",
    "print(relative_x)\n",
    "print(relative_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622520cd-2030-401f-b53f-f0990f9019f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Relative coordinate indices in 2D window\n",
    "relative_coord = relative_x + relative_y\n",
    "relative_coord -= relative_coord[0, -1]\n",
    "print(relative_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fedbb-7aeb-43e7-95f1-cc306db023b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defined function\n",
    "print(fns.relative_position_index(2).reshape((4, 4)))  # 2x2 window\n",
    "print(fns.relative_position_index(3).reshape((9, 9)))  # 3x3 window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275940b-3a94-493b-a4ce-cde58d48f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fns.relative_position_index(6, 2).reshape((36, 4)))  # 6x6 window - 2x2 window\n",
    "print()\n",
    "print(fns.relative_position_index(6, 3).reshape((36, 9)))  # 6x6 window - 3x3 window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b3349-0db3-4795-b42a-7e1f4596d97b",
   "metadata": {},
   "source": [
    "## Multi-head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e29b18-9684-4db1-9c93-bbff8eaa4abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "      Layer (type)              Output Shape         Param #     Tr. Param #\n",
      "=============================================================================\n",
      "          Linear-1         [16, 28, 28, 128]          16,512          16,512\n",
      "          Linear-2         [16, 28, 28, 256]          33,024          33,024\n",
      "         Softmax-3     [16, 4, 7, 7, 16, 16]               0               0\n",
      "          Linear-4         [16, 28, 28, 128]          16,512          16,512\n",
      "=============================================================================\n",
      "Total params: 66,048\n",
      "Trainable params: 66,048\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Multi-head self-attention module\n",
    "msa_module = MultiHeadSelfAttentionLayer(128, 4, 28, 28, 4, True)\n",
    "print(pytorch_model_summary.summary(msa_module, torch.zeros(16, 28, 28, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ccefcbe-1f33-428d-9036-6437db6854cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------\n",
      "                    Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=======================================================================================\n",
      "   MultiHeadSelfAttentionLayer-1     [16, 28, 28, 128]          66,244          66,244\n",
      "   MultiHeadSelfAttentionLayer-2     [16, 28, 28, 128]          66,244          66,244\n",
      "   MultiHeadSelfAttentionLayer-3     [16, 28, 28, 128]          66,244          66,244\n",
      "=======================================================================================\n",
      "Total params: 198,732\n",
      "Trainable params: 198,732\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "modules = fns.clone_layer(msa_module, 3)\n",
    "model = torch.nn.Sequential(*modules)\n",
    "\n",
    "print(pytorch_model_summary.summary(model, torch.zeros(16, 28, 28, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205cc1b6-33fc-49e4-b822-041e5edc6858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "      Layer (type)              Output Shape         Param #     Tr. Param #\n",
      "=============================================================================\n",
      "          Linear-1         [16, 56, 56, 128]          16,512          16,512\n",
      "          Linear-2         [16, 28, 28, 256]          33,024          33,024\n",
      "         Softmax-3     [16, 4, 7, 7, 64, 16]               0               0\n",
      "          Linear-4         [16, 56, 56, 128]          16,512          16,512\n",
      "=============================================================================\n",
      "Total params: 66,048\n",
      "Trainable params: 66,048\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Multi-head attention module\n",
    "sa_module = MultiHeadAttentionLayer(128, 4,\n",
    "                                    56, 56, 8,  # query config\n",
    "                                    28, 28, 4,  # key, value config\n",
    "                                    True)\n",
    "print(pytorch_model_summary.summary(sa_module, torch.zeros(16, 56, 56, 128), torch.zeros(16, 28, 28, 128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743406f5-cc2d-498c-ae9f-290d3dcf8977",
   "metadata": {},
   "source": [
    "## Transformer Bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6987d3-5fbd-427a-bb9d-c26deeed9726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "    EncoderLayer-1     [16, 24, 24, 128]         198,468         198,468\n",
      "    EncoderLayer-2     [16, 24, 24, 128]         198,468         198,468\n",
      "    EncoderLayer-3     [16, 24, 24, 128]         198,468         198,468\n",
      "    EncoderLayer-4     [16, 24, 24, 128]         198,468         198,468\n",
      "    EncoderLayer-5     [16, 24, 24, 128]         198,468         198,468\n",
      "    EncoderLayer-6     [16, 24, 24, 128]         198,468         198,468\n",
      "    EncoderLayer-7     [16, 24, 24, 128]         198,468         198,468\n",
      "    EncoderLayer-8     [16, 24, 24, 128]         198,468         198,468\n",
      "    EncoderLayer-9     [16, 24, 24, 128]         198,468         198,468\n",
      "   EncoderLayer-10     [16, 24, 24, 128]         198,468         198,468\n",
      "   EncoderLayer-11     [16, 24, 24, 128]         198,468         198,468\n",
      "   EncoderLayer-12     [16, 24, 24, 128]         198,468         198,468\n",
      "=========================================================================\n",
      "Total params: 2,381,616\n",
      "Trainable params: 2,381,616\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "encoder = get_transformer_encoder(d_embed=128,\n",
    "                                  positional_encoding=None,\n",
    "                                  relative_position_embedding=True,\n",
    "                                  n_layer=12,\n",
    "                                  n_head=4,\n",
    "                                  d_ff=128*4,\n",
    "                                  n_patch=24,\n",
    "                                  window_size=4)\n",
    "print(pytorch_model_summary.summary(encoder, torch.zeros(16, 24, 24, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c66c14a-ec42-4ea6-8f7e-781a56bb59a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------\n",
      "      Layer (type)                              Input Shape         Param #     Tr. Param #\n",
      "============================================================================================\n",
      "    DecoderLayer-1     [16, 48, 48, 128], [16, 24, 24, 128]         266,516         266,516\n",
      "    DecoderLayer-2     [16, 48, 48, 128], [16, 24, 24, 128]         266,516         266,516\n",
      "    DecoderLayer-3     [16, 48, 48, 128], [16, 24, 24, 128]         266,516         266,516\n",
      "    DecoderLayer-4     [16, 48, 48, 128], [16, 24, 24, 128]         266,516         266,516\n",
      "    DecoderLayer-5     [16, 48, 48, 128], [16, 24, 24, 128]         266,516         266,516\n",
      "    DecoderLayer-6     [16, 48, 48, 128], [16, 24, 24, 128]         266,516         266,516\n",
      "    DecoderLayer-7     [16, 48, 48, 128], [16, 24, 24, 128]         266,516         266,516\n",
      "    DecoderLayer-8     [16, 48, 48, 128], [16, 24, 24, 128]         266,516         266,516\n",
      "    DecoderLayer-9     [16, 48, 48, 128], [16, 24, 24, 128]         266,516         266,516\n",
      "   DecoderLayer-10     [16, 48, 48, 128], [16, 24, 24, 128]         266,516         266,516\n",
      "   DecoderLayer-11     [16, 48, 48, 128], [16, 24, 24, 128]         266,516         266,516\n",
      "   DecoderLayer-12     [16, 48, 48, 128], [16, 24, 24, 128]         266,516         266,516\n",
      "============================================================================================\n",
      "Total params: 3,198,192\n",
      "Trainable params: 3,198,192\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "decoder = get_transformer_decoder(d_embed=128,\n",
    "                                  positional_encoding=None,\n",
    "                                  relative_position_embedding=True,\n",
    "                                  n_layer=12,\n",
    "                                  n_head=4,\n",
    "                                  d_ff=128*4,\n",
    "                                  query_n_patch=48,\n",
    "                                  query_window_size=8,\n",
    "                                  key_n_patch=24,\n",
    "                                  key_window_size=4)\n",
    "\n",
    "print(pytorch_model_summary.summary(decoder,\n",
    "                                    torch.zeros(16, 48, 48, 128), torch.zeros(16, 24, 24, 128),\n",
    "                                    show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23ee4b-cfd2-4384-ab31-2a493c84451e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d860d0cb-e0c8-4187-8072-e3b9928b8571",
   "metadata": {},
   "source": [
    "## Whole SR Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8aaf2e5-435f-4ba4-b464-ad67e2b060dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "            Layer (type)                            Input Shape         Param #     Tr. Param #\n",
      "================================================================================================\n",
      "        EmbeddingLayer-1                         [1, 3, 48, 48]           1,664           1,664\n",
      "    TransformerEncoder-2                       [1, 24, 24, 128]       2,381,616       2,381,616\n",
      "        EmbeddingLayer-3                         [1, 3, 96, 96]           1,664           1,664\n",
      "    TransformerDecoder-4     [1, 48, 48, 128], [1, 24, 24, 128]       3,198,192       3,198,192\n",
      "   ReconstructionBlock-5                       [1, 48, 48, 128]          72,204          72,204\n",
      "================================================================================================\n",
      "Total params: 5,655,340\n",
      "Trainable params: 5,655,340\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# x3 upscale\n",
    "device = torch.device('cuda')\n",
    "srTrans = SRTransformer().to(device)\n",
    "print(pytorch_model_summary.summary(srTrans,\n",
    "                                    torch.zeros(1, 3, 48, 48, device=device), torch.zeros(1, 3, 48*2, 48*2, device=device),\n",
    "                                    show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3c9251-fecf-401f-a51e-7d2c8e1d5faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "            Layer (type)                            Input Shape         Param #     Tr. Param #\n",
      "================================================================================================\n",
      "        EmbeddingLayer-1                         [1, 3, 48, 48]           1,664           1,664\n",
      "    TransformerEncoder-2                       [1, 24, 24, 128]       2,381,616       2,381,616\n",
      "        EmbeddingLayer-3                       [1, 3, 192, 192]           6,272           6,272\n",
      "       OneStageDecoder-4     [1, 48, 48, 128], [1, 24, 24, 128]       3,262,992       3,262,992\n",
      "   ReconstructionBlock-5                       [1, 96, 96, 128]          72,204          72,204\n",
      "================================================================================================\n",
      "Total params: 5,724,748\n",
      "Trainable params: 5,724,748\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# x4 upscale\n",
    "device = torch.device('cuda')\n",
    "srTrans = SRTransformer(upscale=4, decoder_n_layer=12).to(device)\n",
    "print(pytorch_model_summary.summary(srTrans,\n",
    "                                    torch.zeros(1, 3, 48, 48, device=device), torch.zeros(1, 3, 48*4, 48*4, device=device),\n",
    "                                    show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b5a0a1-2864-4df1-8d40-dbdc221890ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "            Layer (type)                            Input Shape         Param #     Tr. Param #\n",
      "================================================================================================\n",
      "        EmbeddingLayer-1                         [1, 3, 48, 48]           1,664           1,664\n",
      "    TransformerEncoder-2                       [1, 24, 24, 128]       2,381,616       2,381,616\n",
      "        EmbeddingLayer-3                       [1, 3, 192, 192]           6,272           6,272\n",
      "       OneStageDecoder-4     [1, 48, 48, 128], [1, 24, 24, 128]       6,459,936       6,459,936\n",
      "   ReconstructionBlock-5                       [1, 96, 96, 128]          72,204          72,204\n",
      "================================================================================================\n",
      "Total params: 8,921,692\n",
      "Trainable params: 8,921,692\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# x4 upscale with 24 decoder layer\n",
    "device = torch.device('cuda')\n",
    "srTrans = SRTransformer(upscale=4, decoder_n_layer=24).to(device)\n",
    "print(pytorch_model_summary.summary(srTrans,\n",
    "                                    torch.zeros(1, 3, 48, 48, device=device), torch.zeros(1, 3, 48*4, 48*4, device=device),\n",
    "                                    show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf6d6a-1f6d-40cc-bd58-815607e1ee4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9af3cd-a4bf-40b8-9930-892ffe9d4754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "            Layer (type)                            Input Shape         Param #     Tr. Param #\n",
      "================================================================================================\n",
      "    TransformerDecoder-1     [1, 48, 48, 128], [1, 24, 24, 128]       1,599,096       1,599,096\n",
      "   ReconstructionBlock-2                       [1, 48, 48, 128]          72,204          72,204\n",
      "        EmbeddingLayer-3                       [1, 3, 192, 192]           1,664           1,664\n",
      "    TransformerDecoder-4     [1, 96, 96, 128], [1, 24, 24, 128]       1,597,848       1,597,848\n",
      "================================================================================================\n",
      "Total params: 3,270,812\n",
      "Trainable params: 3,270,812\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Two-stage decoder for x4 upscale\n",
    "device = torch.device('cuda')\n",
    "twoStageDecoder = TwoStageDecoder(2, 4, 128, 12, 4, 24, 0.1).to(device)\n",
    "\n",
    "x = torch.zeros(1, 24*2, 24*2, 128, device=device)\n",
    "z = torch.zeros(1, 24, 24, 128, device=device)\n",
    "origin_img = torch.zeros(1, 3, 48*2, 48*2, device=device)\n",
    "\n",
    "print(pytorch_model_summary.summary(twoStageDecoder,\n",
    "                                    x, z, origin_img,\n",
    "                                    show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9d7d9d-495e-438b-a5a0-6f85e55964ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "            Layer (type)                           Output Shape         Param #     Tr. Param #\n",
      "================================================================================================\n",
      "        EmbeddingLayer-1                       [1, 24, 24, 128]           1,664           1,664\n",
      "    TransformerEncoder-2                       [1, 24, 24, 128]       2,381,616       2,381,616\n",
      "        EmbeddingLayer-3                       [1, 48, 48, 128]           1,664           1,664\n",
      "       TwoStageDecoder-4     [1, 96, 96, 128], [1, 3, 192, 192]       3,270,812       3,270,812\n",
      "   ReconstructionBlock-5                       [1, 3, 192, 192]          72,204          72,204\n",
      "================================================================================================\n",
      "Total params: 5,727,960\n",
      "Trainable params: 5,727,960\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# x4 upscale with two-stage decoder\n",
    "device = torch.device('cuda')\n",
    "srTrans = SRTransformer(upscale=4, intermediate_upscale=True, decoder_n_layer=12).to(device)\n",
    "print(pytorch_model_summary.summary(srTrans,\n",
    "                                    torch.zeros(1, 3, 48, 48, device=device), torch.zeros(1, 3, 48*2, 48*2, device=device),\n",
    "                                    show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eda30d-44f9-4d39-a055-ae778005a56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdfc5cbb-79ce-4db7-9b3e-4b9abaa1e1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "            Layer (type)                            Input Shape         Param #     Tr. Param #\n",
      "================================================================================================\n",
      "        EmbeddingLayer-1                         [1, 3, 48, 48]           2,340           2,340\n",
      "    TransformerEncoder-2                       [1, 24, 24, 180]       4,696,032       4,696,032\n",
      "                Linear-3                       [1, 24, 24, 180]           8,688           8,688\n",
      "        EmbeddingLayer-4                         [1, 3, 96, 96]           2,340           2,340\n",
      "    TransformerDecoder-5     [1, 48, 48, 180], [1, 24, 24, 180]       6,286,368       6,286,368\n",
      "   ReconstructionBlock-6                       [1, 48, 48, 180]         138,972         138,972\n",
      "================================================================================================\n",
      "Total params: 11,134,740\n",
      "Trainable params: 11,134,740\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# x2 upscale - revised model\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "srTrans = SRTransformer(d_embed=180,\n",
    "                        interpolated_decoder_input=False,\n",
    "                        raw_decoder_input=False).to(device)\n",
    "\n",
    "print(pytorch_model_summary.summary(srTrans,\n",
    "                                    torch.zeros(1, 3, 48, 48, device=device), torch.zeros(1, 3, 48*2, 48*2, device=device),\n",
    "                                    show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccda065d-bd6d-4adb-bdc9-774332b4316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "    EncoderLayer-1     [16, 24, 24, 192]         445,256         445,256\n",
      "    EncoderLayer-2     [16, 24, 24, 192]         445,256         445,256\n",
      "    EncoderLayer-3     [16, 24, 24, 192]         445,256         445,256\n",
      "    EncoderLayer-4     [16, 24, 24, 192]         445,256         445,256\n",
      "    EncoderLayer-5     [16, 24, 24, 192]         445,256         445,256\n",
      "    EncoderLayer-6     [16, 24, 24, 192]         445,256         445,256\n",
      "    EncoderLayer-7     [16, 24, 24, 192]         445,256         445,256\n",
      "    EncoderLayer-8     [16, 24, 24, 192]         445,256         445,256\n",
      "    EncoderLayer-9     [16, 24, 24, 192]         445,256         445,256\n",
      "   EncoderLayer-10     [16, 24, 24, 192]         445,256         445,256\n",
      "   EncoderLayer-11     [16, 24, 24, 192]         445,256         445,256\n",
      "   EncoderLayer-12     [16, 24, 24, 192]         445,256         445,256\n",
      "   EncoderLayer-13     [16, 24, 24, 192]         445,256         445,256\n",
      "   EncoderLayer-14     [16, 24, 24, 192]         445,256         445,256\n",
      "   EncoderLayer-15     [16, 24, 24, 192]         445,256         445,256\n",
      "   EncoderLayer-16     [16, 24, 24, 192]         445,256         445,256\n",
      "   EncoderLayer-17     [16, 24, 24, 192]         445,256         445,256\n",
      "   EncoderLayer-18     [16, 24, 24, 192]         445,256         445,256\n",
      "=========================================================================\n",
      "Total params: 8,014,608\n",
      "Trainable params: 8,014,608\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "encoder = get_transformer_encoder(d_embed=192,\n",
    "                                  positional_encoding=None,\n",
    "                                  relative_position_embedding=True,\n",
    "                                  n_layer=18,\n",
    "                                  n_head=8,\n",
    "                                  d_ff=192*4,\n",
    "                                  n_patch=24,\n",
    "                                  window_size=4)\n",
    "print(pytorch_model_summary.summary(encoder, torch.zeros(16, 24, 24, 192)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f0f93-48a4-4bba-9be9-e7c84a8b296e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SR",
   "language": "python",
   "name": "sr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2cf82bc2e688c3d93a6f9c567c1bf50d23ac93a8e9531158022c12286343afea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
