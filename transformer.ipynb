{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a10862e-db06-4650-afba-ba3fbc622d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coffeetumbler/anaconda3/envs/ano_trans/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import pytorch_model_summary\n",
    "\n",
    "import utils.functions as fns\n",
    "from models.transformer import MultiHeadAttentionLayer\n",
    "from models.transformer import MultiHeadSelfAttentionLayer\n",
    "from models.transformer import get_transformer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34724ee6-9105-495d-bb71-738a5a59d88e",
   "metadata": {},
   "source": [
    "## Window Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a14b44-2a8b-4ec1-9dfc-8747f972e557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4-class cyclic shifting\n",
    "split_heads = torch.arange(72).view(2, 6, 6, 1)\n",
    "split_heads = split_heads.expand(-1, -1, -1, 8)\n",
    "split_heads = split_heads.view(2, 6, 6, 4, 2).permute(0, 3, 1, 2, 4).contiguous()\n",
    "\n",
    "shifted_heads = fns.cyclic_shift(split_heads, 1)\n",
    "# shifted_heads = shifted_heads.permute(0, 2, 3, 1, 4).contiguous().view(2, 6, 6, -1)\n",
    "print(shifted_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168a5e8-108b-49ac-bcf7-1bbbd46c1e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# window partitioning\n",
    "# shifted_heads = shifted_heads.view(2, 6, 6, 4, 2).permute(0, 3, 1, 2, 4).contiguous()\n",
    "partitions = fns.partition_window(shifted_heads, 2)\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1817c246-9f37-482f-97cb-0fa93fba575c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# window merging\n",
    "merged = fns.merge_window(partitions, 2)\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bf687-6d1d-451d-a948-83b504302786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make masking matrix.\n",
    "mask = fns.masking_matrix(4, 6, 6, 2, 1)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47831f-2ec1-4aee-b585-3e4bb1d95a51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Masking\n",
    "attn_values = torch.matmul(partitions, partitions.transpose(-1, -2))\n",
    "attn_values.masked_fill_(mask, -1)\n",
    "print(attn_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f4248-20b9-4e4a-a9d6-2282bcde67b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make masking matrix when query != key.\n",
    "mask = fns.masking_matrix(4, 8, 8, 4, 2,\n",
    "                             4, 4, 2, 1)\n",
    "print(mask[0, 1], '\\n')\n",
    "print(mask[0, 2], '\\n')\n",
    "print(mask[0, 3], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c693c-8674-4ef3-95e7-e9b9c951fda5",
   "metadata": {},
   "source": [
    "## 2D Relative Position Bias (for Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37956e12-957c-4a7b-9661-779293301dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example window index\n",
    "window_size = 3\n",
    "coord_index = np.arange(window_size*window_size).reshape((window_size, window_size))\n",
    "print(coord_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998be6a8-e843-4687-bba5-a18f549a966f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coordinate indices along each axis\n",
    "axis_size = window_size * 2 - 1\n",
    "coord_x = np.repeat(np.arange(window_size) * axis_size, window_size)\n",
    "coord_y = np.tile(np.arange(window_size), window_size)\n",
    "print(coord_x)\n",
    "print(coord_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789fc51-0926-4c98-8b35-3837ba45ddaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Relative coordinate indices along each axis\n",
    "relative_x = coord_x[:, np.newaxis] - coord_x\n",
    "relative_y = coord_y[:, np.newaxis] - coord_y\n",
    "print(relative_x)\n",
    "print(relative_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09112f3b-e885-4408-9b46-53d1b519aa7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Relative coordinate indices in 2D window\n",
    "relative_coord = relative_x + relative_y\n",
    "relative_coord += relative_coord[-1, 0]\n",
    "print(relative_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6275c18-e44f-4c6d-a974-7b062e00b291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defined function\n",
    "print(fns.relative_position_index(2).reshape((4, 4)))  # 2x2 window\n",
    "print(fns.relative_position_index(3).reshape((9, 9)))  # 3x3 window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0bc559-299a-4fbd-9e9b-abd87ea0bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example window index when key != query\n",
    "query_window_size = 4\n",
    "key_window_size = 2\n",
    "qk_ratio = query_window_size // key_window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d34498-000a-4221-9dc7-a73cd0067b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinate indices along each axis\n",
    "axis_size = query_window_size * 2 - qk_ratio\n",
    "\n",
    "query_coord_x = np.repeat(np.arange(query_window_size) * axis_size, query_window_size)\n",
    "query_coord_y = np.tile(np.arange(query_window_size), query_window_size)\n",
    "print(query_coord_x)\n",
    "print(query_coord_y)\n",
    "\n",
    "key_coord_x = np.repeat(np.arange(key_window_size) * axis_size * qk_ratio, key_window_size)\n",
    "key_coord_y = np.tile(np.arange(key_window_size) * qk_ratio, key_window_size)\n",
    "print(key_coord_x)\n",
    "print(key_coord_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ad06d-b822-4059-8453-0f85a0a4e7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Relative coordinate indices along each axis\n",
    "relative_x = query_coord_x[:, np.newaxis] - key_coord_x\n",
    "relative_y = query_coord_y[:, np.newaxis] - key_coord_y\n",
    "print(relative_x)\n",
    "print(relative_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622520cd-2030-401f-b53f-f0990f9019f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Relative coordinate indices in 2D window\n",
    "relative_coord = relative_x + relative_y\n",
    "relative_coord -= relative_coord[0, -1]\n",
    "print(relative_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fedbb-7aeb-43e7-95f1-cc306db023b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defined function\n",
    "print(fns.relative_position_index(2).reshape((4, 4)))  # 2x2 window\n",
    "print(fns.relative_position_index(3).reshape((9, 9)))  # 3x3 window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275940b-3a94-493b-a4ce-cde58d48f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fns.relative_position_index(6, 2).reshape((36, 4)))  # 6x6 window - 2x2 window\n",
    "print()\n",
    "print(fns.relative_position_index(6, 3).reshape((36, 9)))  # 6x6 window - 3x3 window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b3349-0db3-4795-b42a-7e1f4596d97b",
   "metadata": {},
   "source": [
    "## Multi-head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e29b18-9684-4db1-9c93-bbff8eaa4abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "      Layer (type)              Output Shape         Param #     Tr. Param #\n",
      "=============================================================================\n",
      "          Linear-1         [16, 28, 28, 128]          16,512          16,512\n",
      "          Linear-2         [16, 28, 28, 256]          33,024          33,024\n",
      "         Softmax-3     [16, 4, 7, 7, 16, 16]               0               0\n",
      "          Linear-4         [16, 28, 28, 128]          16,512          16,512\n",
      "=============================================================================\n",
      "Total params: 66,048\n",
      "Trainable params: 66,048\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Multi-head self-attention module\n",
    "msa_module = MultiHeadSelfAttentionLayer(128, 4, 28, 28, 4, True)\n",
    "print(pytorch_model_summary.summary(msa_module, torch.zeros(16, 28, 28, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ccefcbe-1f33-428d-9036-6437db6854cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------\n",
      "                    Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=======================================================================================\n",
      "   MultiHeadSelfAttentionLayer-1     [16, 28, 28, 128]          66,244          66,244\n",
      "   MultiHeadSelfAttentionLayer-2     [16, 28, 28, 128]          66,244          66,244\n",
      "   MultiHeadSelfAttentionLayer-3     [16, 28, 28, 128]          66,244          66,244\n",
      "=======================================================================================\n",
      "Total params: 198,732\n",
      "Trainable params: 198,732\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "modules = fns.clone_layer(msa_module, 3)\n",
    "model = torch.nn.Sequential(*modules)\n",
    "\n",
    "print(pytorch_model_summary.summary(model, torch.zeros(16, 28, 28, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205cc1b6-33fc-49e4-b822-041e5edc6858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "      Layer (type)              Output Shape         Param #     Tr. Param #\n",
      "=============================================================================\n",
      "          Linear-1         [16, 56, 56, 128]          16,512          16,512\n",
      "          Linear-2         [16, 28, 28, 256]          33,024          33,024\n",
      "         Softmax-3     [16, 4, 7, 7, 64, 16]               0               0\n",
      "          Linear-4         [16, 56, 56, 128]          16,512          16,512\n",
      "=============================================================================\n",
      "Total params: 66,048\n",
      "Trainable params: 66,048\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Multi-head attention module\n",
    "sa_module = MultiHeadAttentionLayer(128, 4,\n",
    "                                    56, 56, 8,  # query config\n",
    "                                    28, 28, 4,  # key, value config\n",
    "                                    True)\n",
    "print(pytorch_model_summary.summary(sa_module, torch.zeros(16, 56, 56, 128), torch.zeros(16, 28, 28, 128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743406f5-cc2d-498c-ae9f-290d3dcf8977",
   "metadata": {},
   "source": [
    "## Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6987d3-5fbd-427a-bb9d-c26deeed9726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "    EncoderLayer-1     [16, 28, 28, 128]         198,468         198,468\n",
      "    EncoderLayer-2     [16, 28, 28, 128]         198,468         198,468\n",
      "    EncoderLayer-3     [16, 28, 28, 128]         198,468         198,468\n",
      "    EncoderLayer-4     [16, 28, 28, 128]         198,468         198,468\n",
      "    EncoderLayer-5     [16, 28, 28, 128]         198,468         198,468\n",
      "    EncoderLayer-6     [16, 28, 28, 128]         198,468         198,468\n",
      "    EncoderLayer-7     [16, 28, 28, 128]         198,468         198,468\n",
      "    EncoderLayer-8     [16, 28, 28, 128]         198,468         198,468\n",
      "    EncoderLayer-9     [16, 28, 28, 128]         198,468         198,468\n",
      "   EncoderLayer-10     [16, 28, 28, 128]         198,468         198,468\n",
      "   EncoderLayer-11     [16, 28, 28, 128]         198,468         198,468\n",
      "   EncoderLayer-12     [16, 28, 28, 128]         198,468         198,468\n",
      "=========================================================================\n",
      "Total params: 2,381,616\n",
      "Trainable params: 2,381,616\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "encoder = get_transformer_encoder(d_embed=128,\n",
    "                                  positional_encoding=None,\n",
    "                                  relative_position_embedding=True,\n",
    "                                  n_layer=12,\n",
    "                                  n_head=4,\n",
    "                                  d_ff=128*4,\n",
    "                                  n_patch=28,\n",
    "                                  window_size=4)\n",
    "print(pytorch_model_summary.summary(encoder, torch.zeros(16, 28, 28, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66c14a-ec42-4ea6-8f7e-781a56bb59a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ano_trans",
   "language": "python",
   "name": "ano_trans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
