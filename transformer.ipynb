{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a10862e-db06-4650-afba-ba3fbc622d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import pytorch_model_summary\n",
    "\n",
    "import utils.functions as fns\n",
    "from models.transformer import MultiHeadAttentionLayer\n",
    "from models.transformer import get_transformer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34724ee6-9105-495d-bb71-738a5a59d88e",
   "metadata": {},
   "source": [
    "## Window Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a14b44-2a8b-4ec1-9dfc-8747f972e557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4-class cyclic shifting\n",
    "split_heads = torch.arange(72).view(2, 6, 6, 1)\n",
    "split_heads = split_heads.expand(-1, -1, -1, 8)\n",
    "split_heads = split_heads.view(2, 6, 6, 4, 2).permute(0, 3, 1, 2, 4).contiguous()\n",
    "\n",
    "shifted_heads = fns.cyclic_shift(split_heads, 1)\n",
    "# shifted_heads = shifted_heads.permute(0, 2, 3, 1, 4).contiguous().view(2, 6, 6, -1)\n",
    "print(shifted_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168a5e8-108b-49ac-bcf7-1bbbd46c1e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# window partitioning\n",
    "# shifted_heads = shifted_heads.view(2, 6, 6, 4, 2).permute(0, 3, 1, 2, 4).contiguous()\n",
    "partitions = fns.partition_window(shifted_heads, 2)\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1817c246-9f37-482f-97cb-0fa93fba575c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# window merging\n",
    "merged = fns.merge_window(partitions, 2)\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bf687-6d1d-451d-a948-83b504302786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make masking matrix.\n",
    "mask = fns.masking_matrix(4, 6, 6, 2, 1)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47831f-2ec1-4aee-b585-3e4bb1d95a51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Masking\n",
    "attn_values = torch.matmul(partitions, partitions.transpose(-1, -2))\n",
    "attn_values.masked_fill_(mask, -1)\n",
    "print(attn_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c693c-8674-4ef3-95e7-e9b9c951fda5",
   "metadata": {},
   "source": [
    "## 2D Relative Position Bias (for Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37956e12-957c-4a7b-9661-779293301dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example window index\n",
    "window_size = 3\n",
    "coord_index = np.arange(window_size*window_size).reshape((window_size, window_size))\n",
    "print(coord_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998be6a8-e843-4687-bba5-a18f549a966f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coordinate indices along each axis\n",
    "axis_size = window_size * 2 - 1\n",
    "coord_x = np.repeat(np.arange(window_size) * axis_size, window_size)\n",
    "coord_y = np.tile(np.arange(window_size), window_size)\n",
    "print(coord_x)\n",
    "print(coord_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789fc51-0926-4c98-8b35-3837ba45ddaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Relative coordinate indices along each axis\n",
    "relative_x = coord_x[:, np.newaxis] - coord_x\n",
    "relative_y = coord_y[:, np.newaxis] - coord_y\n",
    "print(relative_x)\n",
    "print(relative_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09112f3b-e885-4408-9b46-53d1b519aa7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Relative coordinate indices in 2D window\n",
    "relative_coord = relative_x + relative_y\n",
    "relative_coord += relative_coord[-1, 0]\n",
    "print(relative_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6275c18-e44f-4c6d-a974-7b062e00b291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defined function\n",
    "print(fns.relative_position_index(2).reshape((4, 4)))  # 2x2 window\n",
    "print(fns.relative_position_index(3).reshape((9, 9)))  # 3x3 window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b3349-0db3-4795-b42a-7e1f4596d97b",
   "metadata": {},
   "source": [
    "## Multi-head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e29b18-9684-4db1-9c93-bbff8eaa4abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "      Layer (type)              Output Shape         Param #     Tr. Param #\n",
      "=============================================================================\n",
      "          Linear-1         [16, 28, 28, 768]         197,376         197,376\n",
      "         Softmax-2     [16, 4, 4, 4, 49, 49]               0               0\n",
      "          Linear-3         [16, 28, 28, 256]          65,792          65,792\n",
      "=============================================================================\n",
      "Total params: 263,168\n",
      "Trainable params: 263,168\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "msa_module = MultiHeadAttentionLayer(256, 4, 28, 28, 7, True)\n",
    "print(pytorch_model_summary.summary(msa_module, torch.zeros(16, 28, 28, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ccefcbe-1f33-428d-9036-6437db6854cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\n",
      "                Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "===================================================================================\n",
      "   MultiHeadAttentionLayer-1     [16, 28, 28, 256]         263,844         263,844\n",
      "   MultiHeadAttentionLayer-2     [16, 28, 28, 256]         263,844         263,844\n",
      "   MultiHeadAttentionLayer-3     [16, 28, 28, 256]         263,844         263,844\n",
      "===================================================================================\n",
      "Total params: 791,532\n",
      "Trainable params: 791,532\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "modules = fns.clone_layer(msa_module, 3)\n",
    "model = torch.nn.Sequential(*modules)\n",
    "\n",
    "print(pytorch_model_summary.summary(model, torch.zeros(16, 28, 28, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743406f5-cc2d-498c-ae9f-290d3dcf8977",
   "metadata": {},
   "source": [
    "## Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c6987d3-5fbd-427a-bb9d-c26deeed9726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "    EncoderLayer-1     [16, 28, 28, 256]         790,436         790,436\n",
      "    EncoderLayer-2     [16, 28, 28, 256]         790,436         790,436\n",
      "    EncoderLayer-3     [16, 28, 28, 256]         790,436         790,436\n",
      "    EncoderLayer-4     [16, 28, 28, 256]         790,436         790,436\n",
      "    EncoderLayer-5     [16, 28, 28, 256]         790,436         790,436\n",
      "    EncoderLayer-6     [16, 28, 28, 256]         790,436         790,436\n",
      "=========================================================================\n",
      "Total params: 4,742,616\n",
      "Trainable params: 4,742,616\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "encoder = get_transformer_encoder()\n",
    "print(pytorch_model_summary.summary(encoder, torch.zeros(16, 28, 28, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66c14a-ec42-4ea6-8f7e-781a56bb59a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ano_trans",
   "language": "python",
   "name": "ano_trans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
